{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA JOIN CRIMINAL INCIDENCE AND MIBICI DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Copy of Untitled3.ipynb\n",
    "Automatically generated by Colaboratory.\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1uvNar-cMyzolRTJoddmIUHu3i_2bKjhZ\n",
    "\"\"\"\n",
    "\"\"\" EDA for Join of atasets\n",
    "This script allows to read, clean, analize and join MIBICI and criminalincidence datasets,\n",
    "and show the result of analysis in a histogram. Also, the script does a training model.\n",
    "Read process: \n",
    "    read the files to start the process\n",
    "Clean process:\n",
    "    Delete duplicated rows\n",
    "    Delete Nan-value rows\n",
    "    Delete the rows with a value less than 15 in the diff_seconds column\n",
    "Join files:\n",
    "This file contains the following function:\n",
    "    * create_onedrive_directdownload\n",
    "    * trainModel_scale\n",
    "    * distFrom\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import base64\n",
    "\n",
    "\n",
    "def create_onedrive_directdownload(onedrive_link):\n",
    "    \"\"\"Returns a Oncedrive link for MIBICI dataset\n",
    "        Parameters\n",
    "        ----------\n",
    "            onedrive_link: str\n",
    "            the name of the link\n",
    "        Returns\n",
    "        -------\n",
    "            resultUrl: str\n",
    "        the encoded str link\n",
    "    \"\"\"\n",
    "    data_bytes64 = base64.b64encode(bytes(onedrive_link, 'utf-8'))\n",
    "    data_bytes64_String = data_bytes64.decode('utf-8').replace('/','_').replace('+','-').rstrip(\"=\")\n",
    "    resultUrl = f\"https://api.onedrive.com/v1.0/shares/u!{data_bytes64_String}/root/content\"\n",
    "    return resultUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mibici_dataset_direct_url = create_onedrive_directdownload('https://1drv.ms/u/s!AllbB8dY7-XlonYbKR5K_UxVhb5N?e=2dyg0K')\n",
    "estaciones_direct_url = create_onedrive_directdownload('https://1drv.ms/u/s!AllbB8dY7-XlondVkj-i2oSp7X0M?e=fq1fOv')\n",
    "neighborhoods_lat_lng_direct_url = create_onedrive_directdownload('https://1drv.ms/u/s!AllbB8dY7-XlonigcNaWfXwaC5dC?e=ggEhzD')\n",
    "criminal_incidence_direct_url = create_onedrive_directdownload('https://1drv.ms/u/s!AllbB8dY7-Xlonl9IYCBVwrS6wug?e=8JW4cU')\n",
    "\n",
    "date_cols = ['Inicio_del_viaje', 'Fin_del_viaje']\n",
    "\n",
    "df = pd.read_csv(\n",
    "    mibici_dataset_direct_url,\n",
    "    dtype={\n",
    "        'Anio_de_nacimiento': pd.UInt16Dtype(),\n",
    "        'Origen_Id': pd.UInt16Dtype(),\n",
    "        'Destino_Id': pd.UInt16Dtype(),\n",
    "        'Usuario_Id': pd.UInt32Dtype(),\n",
    "        'Viaje_Id': pd.UInt32Dtype()\n",
    "    },\n",
    "    parse_dates=date_cols,\n",
    "    date_parser=pd.to_datetime\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEANING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar lineas duplicadas ----------\n",
    "df.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# Eliminar lineas completamente vacías --------\n",
    "df.dropna(axis=0, how='all', thresh=None, subset=None, inplace=True)\n",
    "\n",
    "#Sacamos la diferencia en segundos y se agrega a una columna llamada diff_seconds\n",
    "df['diff_seconds'] = df['Fin_del_viaje'] - df['Inicio_del_viaje']\n",
    "df['diff_seconds']= df['diff_seconds']/np.timedelta64(1,'s')\n",
    "\n",
    "#Borramos todo lo que este menos de 15 Segundos en la columna diff_seconds\n",
    "df.drop(df[df.diff_seconds < 15].index, inplace = True)\n",
    "\n",
    "# Filtramos los registros para sólo mujeres\n",
    "df = df[df[\"Genero\"].isin([\"F\"])]\n",
    "\n",
    "estaciones_df = pd.read_csv(estaciones_direct_url)\n",
    "df = df.merge(estaciones_df, left_on='Origen_Id', right_on='id', how= 'left')\n",
    "df = df.merge(estaciones_df, left_on='Destino_Id', right_on='id', how= 'left')\n",
    "df.drop(['id_x', 'id_y'], inplace = True, axis=1)\n",
    "df.rename(columns={\"latitud_x\": \"latitud_origen\", \"longitud_x\": \"longitud_origen\"}, inplace = True)\n",
    "df.rename(columns={\"latitud_y\": \"latitud_destino\", \"longitud_y\": \"longitud_destino\"}, inplace = True)\n",
    "df['mes'] = df['Inicio_del_viaje'].dt.month\n",
    "\n",
    "# ======== Criminal Incidence ========\n",
    "\n",
    "df_criminal = pd.read_csv(criminal_incidence_direct_url)\n",
    "\n",
    "municipalities = [\"GUADALAJARA\", \"ZAPOPAN\", \"SAN PEDRO TLAQUEPAQUE\"]\n",
    "null_values = [\"N.D.\", \"N..D.\"]\n",
    "crimes = [\"LESIONES DOLOSAS\", \"ROBO DE MOTOCICLETA\", \"ROBO A CUENTAHABIENTES\", \"HOMICIDIO DOLOSO\", \"ROBO A NEGOCIO\", \"FEMINICIDIO\"]\n",
    "statuses_to_drop = [\"ZERO_RESULTS\"]\n",
    "\n",
    "df_criminal = df_criminal[df_criminal[\"Municipio\"].isin(municipalities)]\n",
    "df_criminal = df_criminal[~df_criminal[\"Colonia\"].isin(null_values)]\n",
    "df_criminal = df_criminal[df_criminal[\"Delito\"].isin(crimes)]\n",
    "\n",
    "lat_lng = pd.read_csv(neighborhoods_lat_lng_direct_url)\n",
    "lat_lng = lat_lng[~lat_lng[\"status\"].isin(statuses_to_drop)]\n",
    "df_criminal = df_criminal.merge(lat_lng, left_on='Colonia', right_on=\"colonia\", how=\"left\")\n",
    "df_criminal.drop(['Mes', 'Clave_Mun', 'colonia', 'query', 'status'], axis = 1, inplace=True)\n",
    "\n",
    "print(\"Deleting invalid coordinates...\")\n",
    "print(\"Total of rows: \", len(df_criminal.index))\n",
    "indexNames = df_criminal[(df_criminal.location_lat < 20.3257581) | (df_criminal.location_lat > 20.9982375) | (df_criminal.location_lng < -103.6650327) | (df_criminal.location_lng > -103.0809884) ].index\n",
    "df_criminal.drop(indexNames , inplace=True)\n",
    "print(\"Total of rows after deletion: \", len(df_criminal.index))\n",
    "print(\"Deleting invalid coordinates... Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel_scale(model, features, data, nFoldList, nTest, scaler, output):\n",
    "    for i in nFoldList:\n",
    "        y = data[output]\n",
    "        X = data[features]\n",
    "\n",
    "        scaler = Pipeline(steps=[\n",
    "            ('sca', scaler)\n",
    "        ])\n",
    "\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('sca', scaler, features)\n",
    "            ])\n",
    "\n",
    "        my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', model)])\n",
    "\n",
    "        cv = ShuffleSplit( test_size = i, n_splits = nTest )\n",
    "        scores = cross_val_score(my_pipeline, X, y,\n",
    "                              cv = cv,\n",
    "                              scoring='precision')\n",
    "\n",
    "        print(\"nFold\", i)\n",
    "        print(scores)\n",
    "\n",
    "\n",
    "print(\"Test 1: DecisionTreeClassifier - Minmaxscaler...\")\n",
    "features = ['anio', 'mes', 'location_lat', 'location_lng']\n",
    "output = 'y_lesionesDolosas'\n",
    "all_rows = features\n",
    "all_rows.append(output)\n",
    "model = DecisionTreeClassifier()\n",
    "nFoldList = [ 0.25, 0.20, 0.10 ]\n",
    "nTest = 3\n",
    "data = df_criminal[all_rows].dropna()\n",
    "convert_dict = {'anio': float, 'mes': float, 'y_lesionesDolosas': float}\n",
    "data = data.astype(convert_dict)\n",
    "print(\"Total of rows: \", len(data.index))\n",
    "scaler = MinMaxScaler()\n",
    "trainModel_scale(model, features, data, nFoldList, nTest, scaler, output)\n",
    "print(\"Test 1: DecisionTreeClassifier - Minmaxscaler... Done.\")\n",
    "\n",
    "def distFrom(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"Returns the haversine distance between two points\n",
    "        Parameters\n",
    "        ----------\n",
    "            lat1: double\n",
    "                Point 1 latitude \n",
    "            lng1: double\n",
    "                Point 1 longitude\n",
    "            lat2: double\n",
    "                Point 2 latitude \n",
    "            lng2: double\n",
    "                Point 2 longitude\n",
    "        Returns\n",
    "        -------\n",
    "            R * c: double\n",
    "                the haversine distance\n",
    "    \"\"\"\n",
    "\n",
    "    #Radio de la Tierra en km\n",
    "    R = 6373.0\n",
    "\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon1 = np.radians(lng1)\n",
    "    lat2 = np.radians(lat2)\n",
    "    lon2 = np.radians(lng2)\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = (np.sin(dlat/2))**2 + np.cos(lat1) * np.cos(lat2) * (np.sin(dlon/2))**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "\n",
    "neighborhood_location_lat_lng = df_criminal.loc[:, ['Colonia', 'location_lat', 'location_lng']]\n",
    "neighborhood_location_lat_lng.drop_duplicates(subset=['Colonia'], keep=False, inplace=True)\n",
    "\n",
    "stations_with_crime_near = []\n",
    "\n",
    "for _, neighborhood_info in neighborhood_location_lat_lng.iterrows():\n",
    "    for _, station in estaciones_df.iterrows():\n",
    "        dist = distFrom(\n",
    "            float(neighborhood_info['location_lat']),\n",
    "            float(neighborhood_info['location_lng']),\n",
    "            float(station['latitud']),\n",
    "            float(station['longitud'])\n",
    "        )\n",
    "        if dist <= 0.5:\n",
    "            stations_with_crime_near.append(int(station['id']))\n",
    "\n",
    "stations_with_crime_near = set(stations_with_crime_near)\n",
    "\n",
    "stations = []\n",
    "\n",
    "for _, station in estaciones_df.iterrows():\n",
    "    station_id = int(station['id'])\n",
    "    stations.append({'id': station_id, 'hasCrime': station_id in stations_with_crime_near})\n",
    "\n",
    "stations = pd.DataFrame(stations)\n",
    "\n",
    "# Filtramos los registros de MiBici sólo para mujeres\n",
    "df_mibici_gender_filtered = df[df[\"Genero\"].isin([\"F\"])]\n",
    "\n",
    "trips_sum = df_mibici_gender_filtered['Origen_Id'].value_counts() + df_mibici_gender_filtered['Destino_Id'].value_counts()\n",
    "\n",
    "trips_sum = pd.DataFrame(trips_sum)\n",
    "\n",
    "stations_crimes_and_trips = stations.merge(trips_sum, right_index=True, left_on='id', how='right')\n",
    "\n",
    "stations_crimes_and_trips.rename(columns={0: 'tripsSum'}, inplace=True)\n",
    "\n",
    "stations_crimes_and_trips.sort_values(by=\"id\", inplace=True)\n",
    "\n",
    "stations_lat_lng_crimes_trips = stations_crimes_and_trips.merge(\n",
    "    estaciones_df, left_on=\"id\", right_on=\"id\", how=\"left\"\n",
    ")\n",
    "\n",
    "max_trip_count = stations_lat_lng_crimes_trips[\"tripsSum\"].max()\n",
    "\n",
    "stations_lat_lng_crimes_trips[\"size\"] = (\n",
    "    stations_lat_lng_crimes_trips[\"tripsSum\"] / max_trip_count * 400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting data on a Google Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gmplot\n",
    "\n",
    "import gmplot\n",
    "\n",
    "apikey = \"APIKEY\"  # (your API key here)\n",
    "bounds = {\n",
    "    \"north\": estaciones_df.latitud.max(),\n",
    "    \"south\": estaciones_df.latitud.min(),\n",
    "    \"east\": estaciones_df.longitud.max(),\n",
    "    \"west\": estaciones_df.longitud.min(),\n",
    "}\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(\n",
    "    20.669870, -103.352839, 13, apikey=apikey, scale_control=True, fit_bounds=bounds\n",
    ")\n",
    "\n",
    "for _, station in stations_lat_lng_crimes_trips.iterrows():\n",
    "    color = \"red\" if station[\"hasCrime\"] else \"blue\"\n",
    "    gmap.scatter(\n",
    "        [station[\"latitud\"]],\n",
    "        [station[\"longitud\"]],\n",
    "        color=color,\n",
    "        size=station[\"size\"],\n",
    "        marker=False,\n",
    "    )\n",
    "\n",
    "gmap.draw(\"map.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}