{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This nootebook allows to read, clean and analize MIBICI and criminalincidence datasets,\n",
    "also contains a training model that predicts when a crimine might occurs.\n",
    "The process is the following:\n",
    "    Read process: \n",
    "        read the files to start the process\n",
    "    Clean process:\n",
    "        Delete duplicated rows\n",
    "        Delete Nan-value rows\n",
    "        Delete the rows with a value less than 15 in the diff_seconds column\n",
    "    Join files:\n",
    "    \n",
    "--------------------------------------------------------------------------------------    \n",
    "    This file contains the following function:\n",
    "        * create_onedrive_directdownload\n",
    "        * trainAndPredict\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "import base64\n",
    "\n",
    "def create_onedrive_directdownload(onedrive_link):\n",
    "    data_bytes64 = base64.b64encode(bytes(onedrive_link, 'utf-8'))\n",
    "    data_bytes64_String = data_bytes64.decode('utf-8').replace('/','_').replace('+','-').rstrip(\"=\")\n",
    "    resultUrl = f\"https://api.onedrive.com/v1.0/shares/u!{data_bytes64_String}/root/content\"\n",
    "    return resultUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##MiBici\n",
    "mibici_dataset_direct_url = create_onedrive_directdownload('https://1drv.ms/u/s!AllbB8dY7-XlonYbKR5K_UxVhb5N?e=2dyg0K')\n",
    "estaciones_direct_url = create_onedrive_directdownload('https://1drv.ms/u/s!AllbB8dY7-XlondVkj-i2oSp7X0M?e=fq1fOv')\n",
    "\n",
    "date_cols = ['Inicio_del_viaje', 'Fin_del_viaje']\n",
    "\n",
    "print('Loading MiBici data...')\n",
    "df = pd.read_csv(\n",
    "    mibici_dataset_direct_url,\n",
    "    parse_dates=date_cols,\n",
    "    date_parser=pd.to_datetime\n",
    ")\n",
    "print('Done.')\n",
    "\n",
    "# Eliminar lineas duplicadas ----------\n",
    "df.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# Eliminar lineas completamente vacías --------\n",
    "df.dropna(axis=0, how='all', thresh=None, subset=None, inplace=True)\n",
    "\n",
    "#Sacamos la diferencia en segundos y se agrega a una columna llamada diff_seconds\n",
    "df['diff_seconds'] = df['Fin_del_viaje'] - df['Inicio_del_viaje']\n",
    "df['diff_seconds']= df['diff_seconds']/np.timedelta64(1,'s')\n",
    "\n",
    "#Borramos todo lo que este menos de 15 Segundos en la columna diff_seconds\n",
    "df.drop(df[df.diff_seconds < 15].index, inplace = True)\n",
    "\n",
    "estaciones_df = pd.read_csv(estaciones_direct_url)\n",
    "df = df.merge(estaciones_df, left_on='Origen_Id', right_on='id', how= 'left')\n",
    "df = df.merge(estaciones_df, left_on='Destino_Id', right_on='id', how= 'left')\n",
    "\n",
    "del(estaciones_df)\n",
    "\n",
    "df.drop(['id_x', 'id_y'], inplace = True, axis=1)\n",
    "df.rename(columns={\"latitud_x\": \"latitud_origen\", \"longitud_x\": \"longitud_origen\"}, inplace = True)\n",
    "df.rename(columns={\"latitud_y\": \"latitud_destino\", \"longitud_y\": \"longitud_destino\"}, inplace = True)\n",
    "df['mes'] = df['Inicio_del_viaje'].dt.month\n",
    "\n",
    "df['anio'] = df['Inicio_del_viaje'].dt.year\n",
    "df_filtrado = df.drop(['Viaje_Id', 'Usuario_Id', 'Genero', 'Anio_de_nacimiento', 'Inicio_del_viaje', 'Fin_del_viaje', 'Origen_Id', 'Destino_Id', 'diff_seconds'], axis = 1)\n",
    "\n",
    "del(df)\n",
    "\n",
    "df_primero = df_filtrado.loc[:, [ 'latitud_origen', 'longitud_origen', 'mes', 'anio']]\n",
    "df_segundo = df_filtrado.loc[:, [ 'latitud_destino', 'longitud_destino', 'mes', 'anio']]\n",
    "\n",
    "del(df_filtrado)\n",
    "\n",
    "df_primero.rename(columns={'latitud_origen': 'latitud','longitud_origen':'longitud'}, inplace=True)\n",
    "df_segundo.rename(columns={'latitud_destino': 'latitud','longitud_destino':'longitud'}, inplace=True)\n",
    "df_new = pd.concat([df_primero, df_segundo])\n",
    "\n",
    "del(df_primero)\n",
    "del(df_segundo)\n",
    "\n",
    "df_new.rename(columns={'latitud': 'location_lat','longitud':'location_lng'}, inplace=True)\n",
    "X_test = df_new[['anio', 'mes', 'location_lat', 'location_lng']]\n",
    "del(df_new)\n",
    "\n",
    "##Criminal\n",
    "\n",
    "criminal_incidence_direct_url = create_onedrive_directdownload('https://1drv.ms/u/s!AllbB8dY7-Xlonl9IYCBVwrS6wug?e=8JW4cU')\n",
    "neighborhoods_lat_lng_direct_url = create_onedrive_directdownload('https://1drv.ms/u/s!AllbB8dY7-XlonigcNaWfXwaC5dC?e=ggEhzD')\n",
    "\n",
    "print('Loading criminal data...')\n",
    "df_criminal = pd.read_csv(criminal_incidence_direct_url)\n",
    "print('Done.')\n",
    "\n",
    "municipalities = [\"GUADALAJARA\", \"ZAPOPAN\", \"SAN PEDRO TLAQUEPAQUE\"]\n",
    "null_values = [\"N.D.\", \"N..D.\"]\n",
    "crimes = [\"LESIONES DOLOSAS\", \"ROBO DE MOTOCICLETA\", \"ROBO A CUENTAHABIENTES\", \"HOMICIDIO DOLOSO\", \"ROBO A NEGOCIO\", \"FEMINICIDIO\"]\n",
    "statuses_to_drop = [\"ZERO_RESULTS\"]\n",
    "\n",
    "df_criminal = df_criminal[df_criminal[\"Municipio\"].isin(municipalities)]\n",
    "df_criminal = df_criminal[~df_criminal[\"Colonia\"].isin(null_values)]\n",
    "df_criminal = df_criminal[df_criminal[\"Delito\"].isin(crimes)]\n",
    "\n",
    "lat_lng = pd.read_csv(neighborhoods_lat_lng_direct_url)\n",
    "lat_lng = lat_lng[~lat_lng[\"status\"].isin(statuses_to_drop)]\n",
    "df_criminal = df_criminal.merge(lat_lng, left_on='Colonia', right_on=\"colonia\", how=\"left\")\n",
    "df_criminal.drop(['Mes', 'Clave_Mun', 'colonia', 'query', 'status'], axis = 1, inplace=True)\n",
    "\n",
    "df_criminal['y_lesionesDolosas'] = 0\n",
    "df_criminal['y_roboMotocicleta'] = 0\n",
    "df_criminal['y_roboCuentahabientes'] = 0\n",
    "df_criminal['y_homicidioDoloso'] = 0\n",
    "df_criminal['y_roboNegocio'] = 0\n",
    "df_criminal['y_feminicidio'] = 0\n",
    "\n",
    "df_criminal.loc[df_criminal.Delito == 'LESIONES DOLOSAS', 'y_lesionesDolosas'] = 1\n",
    "df_criminal.loc[df_criminal.Delito == 'ROBO DE MOTOCICLETA', 'y_roboMotocicleta'] = 1\n",
    "df_criminal.loc[df_criminal.Delito == 'ROBO A CUENTAHABIENTES', 'y_roboCuentahabientes'] = 1\n",
    "df_criminal.loc[df_criminal.Delito == 'HOMICIDIO DOLOSO', 'y_homicidioDoloso'] = 1\n",
    "df_criminal.loc[df_criminal.Delito == 'ROBO A NEGOCIO', 'y_roboNegocio'] = 1\n",
    "df_criminal.loc[df_criminal.Delito == 'FEMINICIDIO', 'y_feminicidio'] = 1\n",
    "\n",
    "df_criminal.rename(columns={\"Año\": \"anio\", \"Número_mes\": \"mes\"}, inplace=True)\n",
    "\n",
    "print(\"Deleting invalid coordinates...\")\n",
    "print(\"Total of rows: \", len(df_criminal.index))\n",
    "indexNames = df_criminal[(df_criminal.location_lat < 20.3257581) | (df_criminal.location_lat > 20.9982375) | (df_criminal.location_lng < -103.6650327) | (df_criminal.location_lng > -103.0809884) ].index\n",
    "df_criminal.drop(indexNames , inplace=True)\n",
    "print(\"Total of rows after deletion: \", len(df_criminal.index))\n",
    "print(\"Deleting invalid coordinates... Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##trainAndPredict\n",
    "\n",
    "def trainAndPredict(model, features, data, scaler, output, categorical_cols, X_test):\n",
    "    \"\"\"Returns the haversine distance between two points\n",
    "        Parameters\n",
    "        ----------\n",
    "            model:\n",
    "                model for the training\n",
    "            features: Array[str]\n",
    "                features for the model, in other words the vars\n",
    "            data: Dataset\n",
    "                the dataset to train the model\n",
    "            scaler:\n",
    "                scaler for the model MinMax\n",
    "            output: List< >\n",
    "            \n",
    "            categorical_cols: Dataset\n",
    "                colums of the dataset\n",
    "            X_test:\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "            Ry_pred: Array[]\n",
    "                \n",
    "    \"\"\"\n",
    "    y = data[output]\n",
    "    X = data[features]\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    scaler = Pipeline(steps=[\n",
    "        ('sca', scaler) \n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "            ('sca', scaler, features)                    \n",
    "        ])\n",
    "\n",
    "    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', model)])\n",
    "    \n",
    "    print('Model Fitting...')\n",
    "    my_pipeline.fit(X, y)\n",
    "    print('Done.')\n",
    "    \n",
    "    print('Predicting...')\n",
    "    y_pred = my_pipeline.predict(X_test)\n",
    "    print('Done.')\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "output = 'y_homicidioDoloso'\n",
    "output_list = []\n",
    "output_list.append(output)\n",
    "features = ['anio', 'mes', 'location_lat', 'location_lng']\n",
    "all_rows = features + output_list\n",
    "model = DecisionTreeClassifier()\n",
    "data = df_criminal[(df_criminal['y_roboCuentahabientes'] == 1) | (df_criminal['y_homicidioDoloso'] == 1)][all_rows].dropna()\n",
    "scaler = MinMaxScaler()\n",
    "categorical_cols = ['anio', 'mes']\n",
    "convert_dict = {'anio': float, 'mes': float, output: float}\n",
    "data = data.astype(convert_dict) \n",
    "\n",
    "y_pred = trainAndPredict(model, features, data, scaler, output, categorical_cols, X_test)\n",
    "\n",
    "X_test = X_test.assign(y_homicidioDoloso = y_pred)\n",
    "del(y_pred)\n",
    "\n",
    "print('All finished successfully.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}