# -*- coding: utf-8 -*-
"""EDA MiBici.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GlEczc1xrR4VW9-VFL7xajgSIArtXpIh

##Â Downloading dataset and making basic preparations to handle data
"""

import pandas as pd

df = pd.read_csv('https://saturdays-ai-gdl2-plata-mibici.s3-us-west-2.amazonaws.com/data.csv')

df['Anio_de_nacimiento'] = df['Anio_de_nacimiento'].astype(pd.UInt16Dtype())
df['Origen_Id'] = df['Origen_Id'].astype(pd.UInt16Dtype())
df['Destino_Id'] = df['Destino_Id'].astype(pd.UInt16Dtype())
df['Usuario_Id'] = df['Usuario_Id'].astype(pd.UInt32Dtype())
df['Viaje_Id'] = df['Viaje_Id'].astype(pd.UInt32Dtype())
df['Inicio_del_viaje'] = pd.to_datetime(df['Inicio_del_viaje'])
df['Fin_del_viaje'] = pd.to_datetime(df['Fin_del_viaje'])

# upload_to_s3(df)


def upload_to_s3(df):
    """## Upload dataset to an AWS S3 bucket"""
    import s3fs

    s3 = s3fs.S3FileSystem(key='<access-key>', secret='<secret-key>')

    # Use 'w' for py3, 'wb' for py2
    with s3.open('<bucket_name>/<filename>.csv', 'w') as f:
        df.to_csv(f, index=False)
