# -*- coding: utf-8 -*-
"""EDA MiBici.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GlEczc1xrR4VW9-VFL7xajgSIArtXpIh

## Downloading dataset and making basic preparations to handle data
"""

import pandas as pd

date_cols = ['Inicio_del_viaje', 'Fin_del_viaje']

df = pd.read_csv(
    'https://saturdays-ai-gdl2-plata-mibici.s3-us-west-2.amazonaws.com/data.csv',
    dtype={
        'Anio_de_nacimiento': pd.UInt16Dtype(),
        'Origen_Id': pd.UInt16Dtype(),
        'Destino_Id': pd.UInt16Dtype(),
        'Usuario_Id': pd.UInt32Dtype(),
        'Viaje_Id': pd.UInt32Dtype()
    },
    parse_dates=date_cols,
    date_parser=pd.to_datetime
)

# upload_to_s3(df)


def upload_to_s3(df):
    """## Upload dataset to an AWS S3 bucket"""
    import s3fs

    s3 = s3fs.S3FileSystem(key='<access-key>', secret='<secret-key>')

    # Use 'w' for py3, 'wb' for py2
    with s3.open('<bucket_name>/<filename>.csv', 'w') as f:
        df.to_csv(f, index=False)
        
  #Eliminar lineas duplicadas ----------
print(df.shape)
df.head(10)
df.tail()
df_unique= df.drop_duplicates(subset=None, keep='first', inplace=False)
print(df_unique.shape)

#Eliminar lineas completamente vacías --------
df_noempty = df_unique.dropna(axis=0, how='all', thresh=None, subset=None, inplace=False)
print(df_noempty.shape)
